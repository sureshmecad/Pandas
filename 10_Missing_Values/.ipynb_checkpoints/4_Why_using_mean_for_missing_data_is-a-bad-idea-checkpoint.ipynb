{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/why-using-a-mean-for-missing-data-is-a-bad-idea-alternative-imputation-algorithms-837c731c1008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why using a mean for missing data is a bad idea. Alternative imputation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We all know the pain when the dataset we want to use for Machine Learning contains missing data.\n",
    "\n",
    "- The quick and easy workaround is to substitute a **mean for numerical features** and use a **mode for categorical ones**\n",
    "\n",
    "- Even better, someone might just insert 0's or discard the data and proceed to the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and mode ignore feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s have a look at a very simple example to visualize the problem. The following table have 3 variables: Age, Gender and Fitness Score. It shows a Fitness Score results (0–10) performed by people of different age and gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10_Missing_Values](image/2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let’s assume that some of the data in Fitness Score is actually missing, so that after using a mean imputation we can compare results using both tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10_Missing_Values](image/3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imputed values don’t really make sense — in fact, they can have a negative effect on accuracy when training our ML model.\n",
    "  \n",
    "- For example, 78 year old women now has a Fitness Score of 5.1, which is typical for people aged between 42 and 60 years old. \n",
    "\n",
    "- Mean imputation doesn’t take into account a fact that Fitness Score is correlated to Age and Gender features. It only inserts 5.1, a mean of the Fitness Score, while ignoring potential feature correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean reduces a variance of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the previous example, variance of the real Fitness Score and of their mean imputed equivalent will differ. Figure below presents the variance of those two cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10_Missing_Values](image/4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, the variance was reduced (that big change is because the dataset is very small) after using the Mean Imputation. Going deeper into mathematics, a smaller variance leads to the narrower confidence interval in the probability distribution[3]. This leads to nothing else than introducing a bias to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Imputation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fortunately, there is a lot of brilliant alternatives to mean and mode imputations. A lot of them are based on already existing algorithms used for Machine Learning. The following list briefly describes most popular methods, as well as few less known imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE\n",
    "- According to [4], it is the second most popular Imputation method, right after the mean. Initially, a simple imputation is performed (e.g. mean) to replace the missing data for each variable and we also note their positions in the dataset. Then, we take each feature and predict the missing data with Regression model. The remaining features are used as dependent variables for our Regression model. The process is iterated multiple times which updates the imputation values. The common number of iterations is usually 10, but it depends on the dataset. More detailed explanation of the algorithm can be found here[5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "- This popular imputation technique is based on the K-Nearest Neighbours algorithm. For a given instance with missing data, KNN Impute returns n most similar neighbours and replaces the missing element with a mean or mode of the neighbours. The choice between mode and mean depends if the feature is a continuous or a categorical one. Great paper for more in-depth understanding is here[6].\n",
    "\n",
    "#### MissForest\n",
    "- It is a non-standard, but a fairly flexible imputation algorithm. It uses RandomForest at its core to predict the missing data. It can be applied to both continuous and categorical variables which makes it advantageous over other imputation algorithms. Have a look what authors of MissForest wrote about its implementation[7].\n",
    "\n",
    "#### Fuzzy K-means Clustering\n",
    "- It is a less known Imputation technique, but it proves to be more accurate and faster than the basic clustering algorithms according to [8]. It computes the clusters of instances and fills in the missing values which dependns to which cluster the instance with missing data belongs to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
